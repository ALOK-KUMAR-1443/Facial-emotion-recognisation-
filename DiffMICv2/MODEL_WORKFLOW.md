# DiffMICv2 Model Training Workflow

## ğŸ”„ Complete Training Pipeline

### **Phase 1: Setup & Data Loading**
```
1. Clone Repository
   â””â”€> Load project files from GitHub

2. Install Dependencies
   â””â”€> pytorch-lightning, diffusers, einops, etc.

3. Configure Dataset
   â”œâ”€> Set dataroot path
   â”œâ”€> Enable folder structure mode
   â””â”€> Set val_split = 0.2 (20%)

4. Load Data
   â”œâ”€> Read train folder (all classes)
   â”œâ”€> Read test folder (all classes)
   â””â”€> Auto-detect number of classes
```

### **Phase 2: Data Splitting (80/20)**
```
5. Train/Val Split
   â”œâ”€> Get all train samples
   â”œâ”€> Extract labels
   â”œâ”€> Stratified split:
   â”‚   â”œâ”€> 80% â†’ Training set
   â”‚   â””â”€> 20% â†’ Validation set
   â””â”€> Maintain class distribution
```

### **Phase 3: Model Initialization**
```
6. Initialize DiffMIC-v2 Components:
   
   a) Auxiliary Classifier (DCG)
      â”œâ”€> ResNet18 backbone
      â”œâ”€> Global and local predictions
      â”œâ”€> Load pretrained weights
      â””â”€> Freeze (eval mode)
   
   b) Diffusion Model (ConditionalModel)
      â”œâ”€> U-Net architecture
      â”œâ”€> Timestep encoding
      â”œâ”€> Conditional guidance
      â””â”€> Trainable parameters
   
   c) Diffusion Sampler (SR3Sampler)
      â”œâ”€> DDIM scheduler
      â”œâ”€> 1000 train timesteps
      â””â”€> 100 test timesteps
```

### **Phase 4: Training Loop**
```
7. For each epoch (1 to n_epochs):
   
   A. Training Phase:
      â”œâ”€> For each batch in train_loader:
      â”‚   â”‚
      â”‚   â”œâ”€> Load images (x_batch) & labels (y_batch)
      â”‚   â”‚
      â”‚   â”œâ”€> Auxiliary Model Forward (frozen):
      â”‚   â”‚   â”œâ”€> Extract global features
      â”‚   â”‚   â”œâ”€> Extract local patches
      â”‚   â”‚   â”œâ”€> Generate attention maps
      â”‚   â”‚   â””â”€> Predict y0_aux (global & local)
      â”‚   â”‚
      â”‚   â”œâ”€> Diffusion Forward Process:
      â”‚   â”‚   â”œâ”€> Create label map from y_batch
      â”‚   â”‚   â”œâ”€> Add noise at random timestep
      â”‚   â”‚   â”œâ”€> Generate noisy_y
      â”‚   â”‚   â””â”€> Create guided prob map (y0_cond)
      â”‚   â”‚
      â”‚   â”œâ”€> Diffusion Model Forward:
      â”‚   â”‚   â”œâ”€> Input: x_batch, noisy_y, timestep
      â”‚   â”‚   â”œâ”€> Conditions: y0_cond, patches, attentions
      â”‚   â”‚   â””â”€> Predict: noise_pred
      â”‚   â”‚
      â”‚   â”œâ”€> Compute Loss:
      â”‚   â”‚   â”œâ”€> Focal loss with prior weights
      â”‚   â”‚   â””â”€> MSE between noise_pred and noise_gt
      â”‚   â”‚
      â”‚   â””â”€> Backward & Optimize:
      â”‚       â”œâ”€> loss.backward()
      â”‚       â”œâ”€> optimizer.step()
      â”‚       â””â”€> Log train_loss
      â”‚
      â””â”€> Update learning rate (CosineAnnealingLR)
   
   B. Validation Phase (every 5 epochs):
      â”œâ”€> For each batch in val_loader:
      â”‚   â”‚
      â”‚   â”œâ”€> Auxiliary Model Forward
      â”‚   â”œâ”€> Generate y0_cond
      â”‚   â”œâ”€> Sample from noise (yT)
      â”‚   â”‚
      â”‚   â”œâ”€> Diffusion Reverse Process:
      â”‚   â”‚   â”œâ”€> Start with random noise
      â”‚   â”‚   â”œâ”€> Iteratively denoise (100 steps)
      â”‚   â”‚   â”œâ”€> Use DDIM scheduler
      â”‚   â”‚   â””â”€> Generate final prediction
      â”‚   â”‚
      â”‚   â”œâ”€> Average predictions over patches
      â”‚   â””â”€> Store gt & pred
      â”‚
      â”œâ”€> Compute Metrics:
      â”‚   â”œâ”€> Accuracy
      â”‚   â”œâ”€> F1 Score
      â”‚   â”œâ”€> Precision & Recall
      â”‚   â”œâ”€> AUC (one-vs-one)
      â”‚   â””â”€> Cohen's Kappa
      â”‚
      â””â”€> Save Best Checkpoint:
          â””â”€> Monitor F1 score (save_top_k=1)
```

### **Phase 5: Testing**
```
8. Load Best Checkpoint
   â””â”€> Highest F1 score model

9. Test on Test Set:
   â”œâ”€> Similar to validation
   â”œâ”€> Use test_loader
   â””â”€> Report final metrics

10. Generate Results:
    â”œâ”€> Confusion matrix
    â”œâ”€> Class-wise metrics
    â””â”€> Visualizations
```

## ğŸ“Š Data Flow Architecture

```
Input Image (224x224x3)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Auxiliary Classifier  â”‚
    â”‚      (Frozen DCG)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“
    Global Pred    Local Pred
         â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Guided Probability Map â”‚
    â”‚      (y0_cond)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Noisy Label (y_t)     â”‚
    â”‚  (forward diffusion)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Conditional U-Net      â”‚
    â”‚  (Diffusion Model)      â”‚
    â”‚  Inputs: x, y_t, t      â”‚
    â”‚  Conditions: y0_cond    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Noise Prediction (Îµ_Î¸)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Focal Loss + MSE       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Backpropagation & Update
```

## ğŸ¯ Key Components

### **1. Dual-Conditional Guidance**
- **Global Path**: Overall image classification
- **Local Path**: Patch-level predictions
- **Guided Map**: Interpolates between global and local
- **Purpose**: Multi-granularity attention

### **2. Heterologous Diffusion**
- **Forward Process**: Add noise to labels (not images)
- **Reverse Process**: Denoise labels to get predictions
- **Advantage**: Works in latent space, more efficient

### **3. Attention Mechanism**
- **Patches**: Local image regions
- **Attention Maps**: Spatial importance weights
- **Integration**: Guide diffusion process

### **4. Loss Function**
```python
Focal Loss = (1 + Î±(1-p)^Î³) * MSE(noise_pred, noise_gt)
where:
  p = softmax probability
  Î± = 10 (focus on hard samples)
  Î³ = 1 (modulation factor)
```

## ğŸ“ˆ Training Monitoring

**Logged Metrics:**
- `train_loss`: Training loss per batch
- `accuracy`: Validation accuracy
- `f1`: F1 score (used for checkpoint selection)
- `precision`: Precision score
- `recall`: Recall score
- `auc`: AUC one-vs-one
- `kappa`: Cohen's Kappa

**Checkpoints:**
- Saved every epoch
- Best model: Highest F1 score
- Last model: Most recent

## âš™ï¸ Hyperparameters

**Training:**
- Batch size: 8 (Kaggle optimized)
- Epochs: 50 (adjustable)
- Learning rate: 0.001
- Scheduler: CosineAnnealingLR
- Validation frequency: Every 5 epochs

**Diffusion:**
- Train timesteps: 1000
- Test timesteps: 100
- Beta schedule: Linear
- Beta range: [0.0001, 0.02]

**Data:**
- Image size: 224x224
- Normalization: ImageNet stats
- Augmentation: Flip, rotation (train only)

## ğŸ”„ Kaggle-Specific Workflow

```
Cell 1: Clone repo
   â†“
Cell 2: Verify dataset (check train/test folders)
   â†“
Cell 3: Install dependencies
   â†“
Cell 4: Clone EfficientSAM
   â†“
Cell 5: Configure (enable folder structure, set val_split=0.2)
   â†“
Cell 6: Check GPU
   â†“
Cell 7: Run training
   â”œâ”€> Auto 80/20 split
   â”œâ”€> Train with validation
   â””â”€> Save checkpoints
   â†“
Cell 8: View results
   â””â”€> Load checkpoint, display metrics
```

## âœ… Validation Checks

**Before Training:**
- âœ“ Dataset path exists
- âœ“ Train/test folders present
- âœ“ GPU available
- âœ“ Dependencies installed

**During Training:**
- âœ“ Loss decreasing
- âœ“ Validation F1 improving
- âœ“ No OOM errors
- âœ“ Checkpoints saving

**After Training:**
- âœ“ Best checkpoint exists
- âœ“ Metrics computed
- âœ“ Results logged

## ğŸ“ Model Architecture Summary

**DiffMIC-v2 = Auxiliary Classifier + Diffusion Model**

1. **Auxiliary Classifier (DCG)**:
   - Provides prior knowledge
   - Frozen during training
   - Guides diffusion process

2. **Diffusion Model**:
   - Main trainable component
   - Refines predictions
   - Handles uncertainty

3. **Integration**:
   - Dual-conditional guidance
   - Heterologous diffusion
   - Attention-based fusion

## ğŸ“ Output Files

```
/kaggle/working/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.ckpt (highest F1)
â”‚   â””â”€â”€ last.ckpt (latest)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ placental/
â”‚       â””â”€â”€ version_X/
â”‚           â”œâ”€â”€ events.out.tfevents.*
â”‚           â””â”€â”€ hparams.yaml
â””â”€â”€ outputs/
    â””â”€â”€ (saved results)
```

---

**Total Pipeline**: Input Images â†’ Auxiliary Features â†’ Diffusion Process â†’ Final Predictions â†’ Evaluation Metrics
